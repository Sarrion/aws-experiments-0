{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ec2-user/SageMaker')\n",
    "from toniutils import dirtoni as dirt\n",
    "from toniutils import printoni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'engineered_data/experiment-1/'\n",
    "X_train, X_val, Y_train, Y_val = [load(path + i + '.pkl') for i in ('X_train', 'X_val', 'Y_train', 'Y_val')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 317502814057370109, name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 9789628448623312100\n",
       " physical_device_desc: \"device: XLA_CPU device\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import float32 as fl32\n",
    "from tensorflow import random_normal as rnorm\n",
    "from tensorflow import matmul, add\n",
    "from tensorflow.nn import relu,dropout, tanh\n",
    "from tensorflow.initializers import he_uniform, glorot_uniform\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GlorotUniform in module tensorflow.python.ops.init_ops:\n",
      "\n",
      "class GlorotUniform(VarianceScaling)\n",
      " |  The Glorot uniform initializer, also called Xavier uniform initializer.\n",
      " |  \n",
      " |  It draws samples from a uniform distribution within [-limit, limit]\n",
      " |  where `limit` is `sqrt(6 / (fan_in + fan_out))`\n",
      " |  where `fan_in` is the number of input units in the weight tensor\n",
      " |  and `fan_out` is the number of output units in the weight tensor.\n",
      " |  \n",
      " |  Args:\n",
      " |    seed: A Python integer. Used to create random seeds. See\n",
      " |      `tf.set_random_seed`\n",
      " |      for behavior.\n",
      " |    dtype: Default data type, used if no `dtype` argument is provided when\n",
      " |      calling the initializer. Only floating point types are supported.\n",
      " |  \n",
      " |  References:\n",
      " |      [Glorot et al., 2010](http://proceedings.mlr.press/v9/glorot10a.html)\n",
      " |      ([pdf](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf))\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GlorotUniform\n",
      " |      VarianceScaling\n",
      " |      Initializer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, seed=None, dtype=tf.float32)\n",
      " |      DEPRECATED FUNCTION ARGUMENT VALUES\n",
      " |      \n",
      " |      Warning: SOME ARGUMENT VALUES ARE DEPRECATED: `(distribution='normal')`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `normal` is a deprecated alias for `truncated_normal`\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the configuration of the initializer as a JSON-serializable dict.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A JSON-serializable Python dict.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from VarianceScaling:\n",
      " |  \n",
      " |  __call__(self, shape, dtype=None, partition_info=None)\n",
      " |      Returns a tensor object initialized as specified by the initializer.\n",
      " |      \n",
      " |      Args:\n",
      " |        shape: Shape of the tensor.\n",
      " |        dtype: Optional dtype of the tensor. If not provided use the initializer\n",
      " |          dtype.\n",
      " |        partition_info: Optional information about the possible partitioning of a\n",
      " |          tensor.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from Initializer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Instantiates an initializer from a configuration dictionary.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      initializer = RandomUniform(-1, 1)\n",
      " |      config = initializer.get_config()\n",
      " |      initializer = RandomUniform.from_config(config)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        config: A Python dictionary.\n",
      " |          It will typically be the output of `get_config`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An Initializer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Initializer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(glorot_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(shape = [batch_size, X_train.shape[1]], dtype = fl32)\n",
    "labels = tf.placeholder(shape = [batch_size, Y_train.shape[1]], dtype = fl32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant         identity        random_normal        variance_scaling \n",
      "global_variables lecun_normal    random_uniform       zeros            \n",
      "glorot_normal    lecun_uniform   tables_initializer                    \n",
      "glorot_uniform   local_variables truncated_normal                      \n",
      "he_normal        ones            uniform_unit_scaling                  \n",
      "he_uniform       orthogonal      variables                             \n"
     ]
    }
   ],
   "source": [
    "printoni(dirt(tf.initializers), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('l1', reuse = tf.AUTO_REUSE):\n",
    "    w1 = tf.get_variable('w1', [inputs.shape[1].value, 110], initializer = he_uniform())\n",
    "    b1 = tf.get_variable('b1', [110,], initializer = tf.initializers.zeros)\n",
    "    layer1 = dropout( relu(add(matmul(inputs, w1), b1)), rate = 0.1 )\n",
    "\n",
    "with tf.variable_scope('l2', reuse = tf.AUTO_REUSE):\n",
    "    w2 = tf.get_variable('w2', [layer1.shape[1].value, 220], initializer = he_uniform())\n",
    "    b2 = tf.get_variable('b2', [220,], initializer = tf.initializers.zeros)\n",
    "    layer2 = dropout( relu(add(matmul(layer1, w2), b2)), rate = 0.1)\n",
    "\n",
    "with tf.variable_scope('l3', reuse = tf.AUTO_REUSE):\n",
    "    w3 = tf.get_variable('w3', [layer2.shape[1].value, 220], initializer = he_uniform())\n",
    "    b3 = tf.get_variable('b3', [220,], initializer = tf.initializers.zeros)\n",
    "    layer3 = dropout( relu(add(matmul(layer2, w3), b3)), rate = 0.1)\n",
    "\n",
    "with tf.variable_scope('output_layer', reuse = tf.AUTO_REUSE):\n",
    "    w4 = tf.get_variable('w4', [layer2.shape[1].value, 80], initializer = glorot_uniform())\n",
    "    b4 = tf.get_variable('b3', [80,], initializer = tf.initializers.zeros)\n",
    "    layer4 = add(matmul(layer3, w4), b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss', reuse = tf.AUTO_REUSE):\n",
    "    loss = tf.reduce_mean(tf.squared_difference(layer4, labels))\n",
    "    loss_minimize = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  -  val_loss: 0.016354427\n",
      "Epoch: 1  -  val_loss: 0.014080174\n",
      "Epoch: 2  -  val_loss: 0.01342251\n",
      "Epoch: 3  -  val_loss: 0.013251232\n",
      "Epoch: 4  -  val_loss: 0.013097901\n",
      "Epoch: 5  -  val_loss: 0.0130063845\n",
      "Epoch: 6  -  val_loss: 0.01294462\n",
      "Epoch: 7  -  val_loss: 0.012827262\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for i in range(X_train.shape[0] // batch_size):\n",
    "        sess.run([loss, loss_minimize], \n",
    "                 feed_dict = {inputs: X_train[i * batch_size:(i + 1)*batch_size, :],\n",
    "                              labels: Y_train[i * batch_size:(i + 1)*batch_size, :]})\n",
    "\n",
    "    test_loss = []   \n",
    "    for i in range(X_val.shape[0] // batch_size):\n",
    "        new_loss,_ = sess.run([loss, loss_minimize], \n",
    "                 feed_dict = {inputs: X_val[i * batch_size:(i + 1)*batch_size, :],\n",
    "                              labels: Y_val[i * batch_size:(i + 1)*batch_size, :]})\n",
    "        test_loss.append(new_loss)\n",
    "\n",
    "    import numpy as np\n",
    "    mean_test_loss = np.array(test_loss).mean()\n",
    "    print('Epoch: ' + str(epoch) + '  -  val_loss: ' + str(mean_test_loss))\n",
    "\n",
    "print(sess.graph)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013327003"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0135560045"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012763917"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0126214735"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_test_loss # 20 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Save Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 4205122772179942051, name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 8769741372308327298\n",
       " physical_device_desc: \"device: XLA_CPU device\"]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.io.write_graph(sess.graph, '/tmp/my-model', 'train.pbtxt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
